{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Our Chatbot using llama-index + ChatGPT API**\n",
        "In this notebook we will be building a chatbot from your own documents. This can be your company documents, FAQs product manuals etc. <br>\n",
        "You will need a ChatGPT API key. Dont worry you can get a free trial from which will be more than enough to train and test this model. <br>\n",
        "ChatGPT API: https://platform.openai.com/ <br>\n",
        "llama-index documentation: https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#optional-save-the-index-for-future-use"
      ],
      "metadata": {
        "id": "jmrkour1IY0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required packages"
      ],
      "metadata": {
        "id": "K6h3Xz3cJT1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install langchain\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "id": "QgEp-cOkl6bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages "
      ],
      "metadata": {
        "id": "V56AFwj4meWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufKqA4T5lxsd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain import OpenAI\n",
        "from llama_index import (\n",
        "    GPTVectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    LLMPredictor,\n",
        "    ServiceContext,\n",
        "    QuestionAnswerPrompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up OpenAI API Key \n",
        "You can get your OpenAI API key from the API website i gave earlier. <br>\n",
        "Replace \"YOUR_OPENAI_API_KEY\" with your own key"
      ],
      "metadata": {
        "id": "Kdc2g2LLJuuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR_OPENAI_API_KEY'"
      ],
      "metadata": {
        "id": "ljMOIXTqmr3p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load your documents\n",
        "Load your documents this can be JSON , word or txt format <br>\n",
        "NOTE: This is link to a **folder** containing your documents. For my case my documents are in directory named data"
      ],
      "metadata": {
        "id": "811VJ4_4Kb5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('/content/data').load_data()"
      ],
      "metadata": {
        "id": "yvjkwMFUnQtG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Customizing Large Language Model's\n",
        "By default, we use OpenAI's text-davinci-003 model. You may choose to use another LLM when constructing an index. <br>\n",
        "We set the temperature to 0 to ensure that the responses are always the same."
      ],
      "metadata": {
        "id": "GGJuLLCpK7zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))\n"
      ],
      "metadata": {
        "id": "x2hxY1iqncjJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up Global Servic Context\n",
        "We are using the Service Context class to define the parameters for the chatbot service. This service context will always be used as the default if not specified as a keyword argument in LlamaIndex functions."
      ],
      "metadata": {
        "id": "tX3sbiGdLimU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configure service context\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "# build index\n",
        "index = GPTVectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "gV0_Tbhb74lR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Index\n",
        "Save index so that we can reuse it without recreating it. By default, data is stored in-memory. To save it a directory specify the name of the directory it will be stored there"
      ],
      "metadata": {
        "id": "XGbJj5bDLy8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.storage_context.persist(persist_dir=\"index\")"
      ],
      "metadata": {
        "id": "5me50X8a8Asx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Index\n",
        "To re- load the index from disk use **load_index_from_storage() ** method."
      ],
      "metadata": {
        "id": "hQ_rdvVRMWSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import StorageContext, load_index_from_storage\n",
        "\n",
        "# rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"index\")\n",
        "\n",
        "# load index\n",
        "index = load_index_from_storage(storage_context)"
      ],
      "metadata": {
        "id": "R8NRPUS58HmC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Chatbot \n",
        "After building the index, you can now query it with a QueryEngine. Note that a “query” is simply an input to an LLM - this means that you can use the index for question-answering."
      ],
      "metadata": {
        "id": "tKycXA8_Mq95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"Explain the English Rule\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxysD5Cm8oKw",
        "outputId": "449c8ba7-62cf-46cd-edf8-ea6e08a79aa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The English Rule in New York City refers to the period of time when the city was under the control of the English monarchy. This period began in 1664 when the English captured the Dutch colony of New Amsterdam and renamed it New York after the Duke of York (the future King James II and VII). During this period, the English established a strong presence in the city, with the British military and political base of operations in North America. The English also established a system of slavery in the city, with 42% of households enslaving Africans by 1730. The English also established the Stamp Act Congress in 1765, which was a meeting of representatives from the colonies to discuss taxation. The English Rule in New York City ended in 1783 when the British forces evacuated the city at the close of the American Revolutionary War.\n"
          ]
        }
      ]
    }
  ]
}